import json
from typing import Dict, Any, List
import numpy as np
from openai import OpenAI
from sentence_transformers import SentenceTransformer, util


class Judge:
    def __init__(self, client: OpenAI):
        self.client = client
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

    async def g_eval(self, prompt: str, task: str, response: str) -> Dict[str, Any]:
        eval_prompt = f"""
        Your role is to evaluate the quality of a response generated by an LLM to a given prompt based on the following task: {task}.

        Criteria:
        1. Relevance: Does the response adequately address the prompt (Specifically in relation to the task: {task})? (1-10)
        2. Accuracy: Is the response factually correct? (1-10)
        3. Coherence: Is the response grammatically correct and logically structured? (1-10)
        4. Fluency: Is the response smooth and readable? (1-10)

        Input Prompt: {prompt}

        Generated Response: {response}

        Please strictly return a JSON object with scores and reasoning in a format as follows:
        {{
            "relevance": {{
                "score": <number>,
                "reasoning": "<text>"
            }},
            "accuracy": {{
                "score": <number>,
                "reasoning": "<text>"
            }},
            "coherence": {{
                "score": <number>,
                "reasoning": "<text>"
            }},
            "fluency": {{
                "score": <number>,
                "reasoning": "<text>"
            }}
        }}
        
        """
        try:
            completion = self.client.chat.completions.create(
                model= "gpt-4o",
                messages = [{"role": "user", "content": eval_prompt}]
            )
            raw_response = completion.choices[0].message.content
            raw_response = raw_response.replace('json', '')
            raw_response = raw_response.replace('```', '')
            try:
                evaluation = json.loads( raw_response )
            except Exception as e:
                return {"error": str(e) }
            return evaluation
        
        except json.JSONDecodeError:
            return {"error": "Failed to parse evaluation JSON."}
        except Exception as e:
            return {"error": str(e)}

    async def comparative_analysis(self, responses: Dict[str, str], input_prompt: str) -> Dict[str, Any]:
        scores = {}
        evaluations = {}
        best_model = None
        best_score = -np.inf

        for model, response in responses.items():
            evaluation = await self.g_eval(input_prompt, response)
            if "error" in evaluation:
                evaluations[model] = evaluation
                continue

            # Compute average score across criteria
            avg_score = np.mean([
                evaluation[criterion]["score"]
                for criterion in ["relevance", "accuracy", "coherence", "fluency"]
            ])
            scores[model] = avg_score
            evaluations[model] = evaluation

            # Update best model
            if avg_score > best_score:
                best_model = model
                best_score = avg_score


        for key, value in scores.items():
            scores[key] = float(value)
        best_score = float(best_score)
        scores = {key: float(value) for key, value in scores.items()}

        return {
            "best_model": best_model,
            "best_score": best_score,
            "evaluations": evaluations,
            "scores": scores,
        }

    # def diversity_check(self, responses: Dict[str, str]) -> float:
    #     embeddings = self.embedding_model.encode(list(responses.values()))
    #     similarities = util.pytorch_cos_sim(embeddings, embeddings)
    #     avg_similarity = (similarities.sum() - len(responses)) / (len(responses) * (len(responses) - 1))
    #     return 1 - avg_similarity.item()  # Higher score indicates greater diversity

    # def length_analysis(self, responses: Dict[str, str]) -> Dict[str, Any]:
    #     lengths = {model: len(response.split()) for model, response in responses.items()}
    #     avg_length = np.mean(list(lengths.values()))
    #     std_length = np.std(list(lengths.values()))
    #     return {"lengths": lengths, "average": avg_length, "std_dev": std_length}
